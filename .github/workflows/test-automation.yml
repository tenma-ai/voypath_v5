name: 🧪 Automated Testing Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'project/src/**'
      - 'project/tests/**'
      - 'project/supabase/**'
      - 'project/package.json'
      - 'project/vitest.config.ts'
  pull_request:
    branches: [ main ]
    paths:
      - 'project/src/**'
      - 'project/tests/**'
      - 'project/supabase/**'
      - 'project/package.json'
      - 'project/vitest.config.ts'
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
        - unit
        - integration
        - performance
        - all

env:
  NODE_VERSION: '18'
  SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
  SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
  SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

jobs:
  # Fast unit tests - run first
  unit-tests:
    name: 🔬 Unit Tests
    runs-on: ubuntu-latest
    if: github.event_name != 'schedule'
    
    steps:
    - name: 📚 Checkout code
      uses: actions/checkout@v4

    - name: 📦 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: project/package-lock.json

    - name: 📥 Install dependencies
      working-directory: project
      run: npm ci

    - name: 🔍 Run unit tests
      working-directory: project
      run: npm run test:unit
      env:
        NODE_ENV: test

    - name: 📊 Upload unit test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results
        path: project/tests/results/
        retention-days: 7

  # Integration tests - run after unit tests pass
  integration-tests:
    name: 🔗 Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: success() && github.event_name != 'schedule'
    
    steps:
    - name: 📚 Checkout code
      uses: actions/checkout@v4

    - name: 📦 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: project/package-lock.json

    - name: 📥 Install dependencies
      working-directory: project
      run: npm ci

    - name: 🔗 Run integration tests
      working-directory: project
      run: npm run test:integration
      env:
        NODE_ENV: test
        INTEGRATION_TEST_TIMEOUT: 60000

    - name: 📊 Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: project/tests/results/
        retention-days: 7

  # Performance tests - run separately or on schedule
  performance-tests:
    name: ⚡ Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'all'
    
    steps:
    - name: 📚 Checkout code
      uses: actions/checkout@v4

    - name: 📦 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: project/package-lock.json

    - name: 📥 Install dependencies
      working-directory: project
      run: npm ci

    - name: ⚡ Run performance tests
      working-directory: project
      run: npm run test:performance
      env:
        NODE_ENV: test
        PERFORMANCE_TEST_ITERATIONS: 20
        PERFORMANCE_THRESHOLD_MS: 500

    - name: 📊 Upload performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: project/tests/results/
        retention-days: 30

    - name: 📈 Performance regression check
      working-directory: project
      run: |
        echo "🔍 Checking for performance regressions..."
        # Compare with baseline metrics
        node -e "
          const fs = require('fs');
          const results = JSON.parse(fs.readFileSync('tests/results/test-results.json', 'utf8'));
          console.log('Performance summary:', results.performance);
        " || echo "No performance results found"

  # Code coverage - comprehensive analysis
  coverage:
    name: 📊 Code Coverage
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: success() && github.event_name != 'schedule'
    
    steps:
    - name: 📚 Checkout code
      uses: actions/checkout@v4

    - name: 📦 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: project/package-lock.json

    - name: 📥 Install dependencies
      working-directory: project
      run: npm ci

    - name: 📊 Generate coverage report
      working-directory: project
      run: npm run test:coverage
      env:
        NODE_ENV: test

    - name: 📤 Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        directory: project/coverage
        flags: place-api
        name: place-api-coverage
        fail_ci_if_error: true

    - name: 📊 Upload coverage artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-report
        path: project/coverage/
        retention-days: 30

  # Quality gates - final checks
  quality-gates:
    name: 🚪 Quality Gates
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, coverage]
    if: always() && github.event_name != 'schedule'
    
    steps:
    - name: 📚 Checkout code
      uses: actions/checkout@v4

    - name: 📦 Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: project/package-lock.json

    - name: 📥 Install dependencies
      working-directory: project
      run: npm ci

    - name: 📥 Download test artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: '*-test-results'
        path: project/tests/results/
        merge-multiple: true

    - name: 🚪 Check quality gates
      working-directory: project
      run: |
        echo "🔍 Checking quality gates..."
        
        # Check test results
        UNIT_PASS=$(cat tests/results/test-results.json 2>/dev/null | jq -r '.passRate // 0')
        echo "Unit test pass rate: $UNIT_PASS%"
        
        # Quality gate thresholds
        MIN_PASS_RATE=95
        MIN_COVERAGE=80
        
        if (( $(echo "$UNIT_PASS < $MIN_PASS_RATE" | bc -l) )); then
          echo "❌ Quality gate failed: Pass rate $UNIT_PASS% < $MIN_PASS_RATE%"
          exit 1
        fi
        
        echo "✅ All quality gates passed!"

    - name: 📝 Create test summary
      if: always()
      run: |
        echo "## 🧪 Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | Status | Details |" >> $GITHUB_STEP_SUMMARY
        echo "|------------|---------|---------|" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} | Core functionality testing |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result == 'success' && '✅ Passed' || '❌ Failed' }} | End-to-end workflow testing |" >> $GITHUB_STEP_SUMMARY
        echo "| Code Coverage | ${{ needs.coverage.result == 'success' && '✅ Passed' || '❌ Failed' }} | Test coverage analysis |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "🔗 [View detailed test reports in artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY

  # Notification job for failures
  notify-failure:
    name: 📢 Notify on Failure
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, quality-gates]
    if: failure() && github.ref == 'refs/heads/main'
    
    steps:
    - name: 📢 Send Slack notification
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        fields: repo,message,commit,author,action,eventName,ref,workflow
        text: '🚨 Test pipeline failed on main branch'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      if: env.SLACK_WEBHOOK_URL != ''

    - name: 📧 Create GitHub issue on repeated failures
      uses: actions/github-script@v7
      if: github.event_name == 'push'
      with:
        script: |
          const title = '🚨 Automated Test Failure Alert'
          const body = `
          ## Test Pipeline Failure
          
          **Branch:** ${context.ref}
          **Commit:** ${context.sha}
          **Workflow:** ${context.workflow}
          **Run:** https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}
          
          Please investigate the test failures and fix them promptly.
          
          **Failing Jobs:**
          - Unit Tests: ${{ needs.unit-tests.result }}
          - Integration Tests: ${{ needs.integration-tests.result }}
          - Quality Gates: ${{ needs.quality-gates.result }}
          `
          
          // Check if similar issue already exists
          const issues = await github.rest.issues.listForRepo({
            owner: context.repo.owner,
            repo: context.repo.repo,
            state: 'open',
            labels: 'automated-test-failure'
          })
          
          if (issues.data.length === 0) {
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: title,
              body: body,
              labels: ['automated-test-failure', 'bug', 'priority-high']
            })
          }